import time, json, os, re
from datetime import datetime
from dateutil import tz
from bs4 import BeautifulSoup
from urllib.parse import urlparse
from selenium import webdriver
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.common.by import By
from webdriver_manager.chrome import ChromeDriverManager


# S3 ì„¤ì •
S3_BUCKET_NAME = "swpp-12-bucket"
S3_REGION = "ap-northeast-2"

# S3 í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”
try:
    import boto3
    from botocore.exceptions import ClientError
    s3_client = boto3.client('s3', region_name=S3_REGION)
except:
    s3_client = None


SECTION_URL = "https://news.naver.com/section/101"  # ê²½ì œ ì„¹ì…˜
TARGET_COUNT = 1000


def setup_driver():
    import os, subprocess

    options = Options()
    options.add_argument("--headless=new")
    options.add_argument("--no-sandbox")
    options.add_argument("--disable-dev-shm-usage")
    options.add_argument("--disable-gpu")
    options.add_argument("--disable-extensions")
    options.add_argument("--disable-logging")
    options.add_argument("--log-level=3")
    options.add_argument(
        "user-agent=Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) "
        "AppleWebKit/537.36 (KHTML, like Gecko) "
        "Chrome/120.0.0.0 Safari/537.36"
    )
    options.add_argument("--window-size=1920,1080")
    options.add_argument("--lang=ko-KR")

    chrome_bin_candidates = [
        os.environ.get("CHROME_BIN"),
        "/Applications/Google Chrome.app/Contents/MacOS/Google Chrome",
        "/Applications/Chromium.app/Contents/MacOS/Chromium",
        "/opt/google/chrome/chrome",
        "/usr/bin/google-chrome",
        "/usr/bin/google-chrome-stable",
        "/usr/bin/chromium",
        "/usr/bin/chromium-browser",
    ]
    chrome_bin = next((p for p in chrome_bin_candidates if p and os.path.exists(p)), None)
    if not chrome_bin:
        raise RuntimeError("Chrome binary not found")

    if chrome_bin.endswith(".app/Contents/MacOS/Google Chrome") or chrome_bin.endswith(".app/Contents/MacOS/Chromium"):
        real = chrome_bin
    else:
        real = os.path.realpath(chrome_bin)

    options.binary_location = real

    try:
        ver = subprocess.run([real, "--version"], capture_output=True, text=True)
        print("[BROWSER]", ver.stdout.strip() or real)
    except Exception:
        print("[BROWSER] Using", real)

    driver_path = ChromeDriverManager().install()
    service = Service(driver_path)
    driver = webdriver.Chrome(service=service, options=options)
    driver.set_page_load_timeout(60)
    driver.implicitly_wait(3)
    
    caps = driver.capabilities
    print(f"[DRV] Chrome {caps.get('browserVersion')} / Driver {caps.get('chrome',{}).get('chromedriverVersion','').split(' ')[0]}")
    
    return driver


def extract_content(driver, url):
    """ê¸°ì‚¬ ë³¸ë¬¸ ë° ë°œí–‰ì¼ ì¶”ì¶œ"""
    try:
        driver.get(url)
        time.sleep(1.5)

        html = driver.page_source
        soup = BeautifulSoup(html, "lxml")

        content_text = None
        published_at = None

        # ë„¤ì´ë²„ ë‰´ìŠ¤ì¸ ê²½ìš°
        if "news.naver.com" in url:
            # ë°œí–‰ì¼ ì¶”ì¶œ
            date_elem = soup.select_one("span.media_end_head_info_datestamp_time")
            if date_elem:
                date_text = date_elem.get("data-date-time") or date_elem.get_text(strip=True)
                try:
                    # ISO í˜•ì‹ìœ¼ë¡œ ë³€í™˜
                    from dateutil import parser
                    dt = parser.parse(date_text)
                    published_at = dt.strftime("%a, %d %b %Y %H:%M:%S GMT")
                except:
                    published_at = date_text
            
            # ë³¸ë¬¸ ì¶”ì¶œ
            content_area = soup.find("article", id="dic_area")
            if not content_area:
                content_area = soup.find("div", id="articleBodyContents")
            
            if content_area:
                for tag in content_area(['script', 'style', 'em', 'strong']):
                    tag.decompose()
                
                text = content_area.get_text(" ", strip=True)
                text = re.sub(r'\s+', ' ', text).strip()
                text = re.sub(r'// flash.*', '', text)
                text = re.sub(r'ë¬´ë‹¨ì „ì¬.*', '', text)
                text = re.sub(r'â“’.*', '', text)
                
                if len(text) >= 100:
                    content_text = text
        
        # ì¼ë°˜ ì–¸ë¡ ì‚¬ ì‚¬ì´íŠ¸
        if not content_text:
            for tag in soup(['script', 'style', 'nav', 'header', 'footer', 'aside']):
                tag.decompose()

            content_area = soup.find("article")
            if not content_area:
                content_area = soup.find("main")
            
            if content_area:
                text = content_area.get_text(" ", strip=True)
            else:
                paragraphs = soup.find_all("p")
                text = " ".join(p.get_text(" ", strip=True) for p in paragraphs)
            
            text = re.sub(r'\s+', ' ', text).strip()

            if len(text) >= 100:
                content_text = text
        
        return content_text, published_at
    except Exception as e:
        return None, None


# ë„¤ì´ë²„ ë‰´ìŠ¤ ì–¸ë¡ ì‚¬ ì½”ë“œ ë§¤í•‘ (ì£¼ìš” ì–¸ë¡ ì‚¬)
NAVER_PRESS_CODE = {
    "001": "ì—°í•©ë‰´ìŠ¤", "003": "ë‰´ì‹œìŠ¤", "005": "êµ­ë¯¼ì¼ë³´", "008": "ë¨¸ë‹ˆíˆ¬ë°ì´",
    "009": "ë§¤ì¼ê²½ì œ", "011": "ì„œìš¸ê²½ì œ", "014": "íŒŒì´ë‚¸ì…œë‰´ìŠ¤", "015": "í•œêµ­ê²½ì œ",
    "016": "í—¤ëŸ´ë“œê²½ì œ", "018": "ì´ë°ì¼ë¦¬", "020": "ë™ì•„ì¼ë³´", "021": "ë¬¸í™”ì¼ë³´",
    "022": "ì„¸ê³„ì¼ë³´", "023": "ì¡°ì„ ì¼ë³´", "025": "ì¤‘ì•™ì¼ë³´", "028": "í•œê²¨ë ˆ",
    "032": "ê²½í–¥ì‹ ë¬¸", "038": "í•œêµ­ì¼ë³´", "047": "ì˜¤ë§ˆì´ë‰´ìŠ¤", "052": "YTN",
    "055": "SBS", "056": "MBC", "057": "MBN", "214": "MBN",
    "081": "ì„œìš¸ì‹ ë¬¸", "082": "ë¶€ì‚°ì¼ë³´", "083": "ë§¤ì¼ì‹ ë¬¸", "084": "êµ­ì œì‹ ë¬¸",
    "087": "ê°•ì›ì¼ë³´", "088": "ì „ë¶ì¼ë³´", "092": "ë™ì•„ì¼ë³´",
    "119": "ë°ì¼ë¦¬ì•ˆ", "123": "ì¡°ì„¸ì¼ë³´", "138": "ë””ì§€í„¸íƒ€ì„ìŠ¤", "243": "ì´ì½”ë…¸ë¯¸ìŠ¤íŠ¸",
    "277": "ì•„ì‹œì•„ê²½ì œ", "293": "ë¸”ë¡œí„°", "366": "ì¡°ì„ ë¹„ì¦ˆ", "374": "SBS Biz",
    "417": "ë¨¸ë‹ˆS", "421": "ë‰´ìŠ¤1", "422": "ì—°í•©ì¸í¬ë§¥ìŠ¤", "449": "ì±„ë„A",
    "629": "ë”íŒ©íŠ¸", "648": "ë¹„ì¦ˆì›Œì¹˜", "654": "NSPí†µì‹ ",
}


def extract_source(url: str) -> str:
    """URLì—ì„œ ì–¸ë¡ ì‚¬ëª… ì¶”ì¶œ"""
    try:
        # ë„¤ì´ë²„ ë‰´ìŠ¤ì¸ ê²½ìš°
        if "news.naver.com" in url:
            parts = url.split("/")
            if "article" in parts or "mnews/article" in url:
                # URL íŒ¨í„´: .../article/629/0000445887 ë˜ëŠ” .../mnews/article/629/...
                for i, part in enumerate(parts):
                    if part in ["article"] and i + 1 < len(parts):
                        press_code = parts[i + 1]
                        return NAVER_PRESS_CODE.get(press_code, f"press_{press_code}")
        
        # ì¼ë°˜ ì–¸ë¡ ì‚¬ ì‚¬ì´íŠ¸
        host = (urlparse(url).hostname or "").lower()
        if host.startswith("www."):
            host = host[4:]
        
        # ë„ë©”ì¸ë³„ ë§¤í•‘
        domain_map = {
            "chosun": "ì¡°ì„ ì¼ë³´",
            "donga": "ë™ì•„ì¼ë³´",
            "joongang": "ì¤‘ì•™ì¼ë³´",
            "hankyung": "í•œêµ­ê²½ì œ",
            "mk": "ë§¤ì¼ê²½ì œ",
            "edaily": "ì´ë°ì¼ë¦¬",
            "fnnews": "íŒŒì´ë‚¸ì…œë‰´ìŠ¤",
            "asiae": "ì•„ì‹œì•„ê²½ì œ",
            "sedaily": "ì„œìš¸ê²½ì œ",
            "heraldcorp": "í—¤ëŸ´ë“œê²½ì œ",
            "mt": "ë¨¸ë‹ˆíˆ¬ë°ì´",
        }
        
        labels = [p for p in host.split(".") if p]
        if len(labels) >= 2:
            domain_key = labels[-2]
            return domain_map.get(domain_key, domain_key)
        
        return host
    except:
        return "Unknown"


def crawl_economy_section(driver, target_count=TARGET_COUNT):
    """ë„¤ì´ë²„ ê²½ì œ ì„¹ì…˜ì—ì„œ ê¸°ì‚¬ URL ìˆ˜ì§‘"""
    print("\n" + "="*60)
    print("ğŸ“° ê¸°ì‚¬ URL ìˆ˜ì§‘ ì‹œì‘")
    print("="*60)
    
    driver.get(SECTION_URL)
    time.sleep(3)

    seen_urls = set()
    articles = []

    click_try = 0
    max_clicks = 200
    no_new_rounds = 0

    while len(articles) < target_count and click_try < max_clicks:
        html = driver.page_source
        soup = BeautifulSoup(html, "lxml")

        # ê¸°ì‚¬ ë¸”ë¡: li.sa_item
        blocks = soup.select("li.sa_item")

        before = len(articles)

        for li in blocks:
            a = li.select_one("a.sa_text_title")
            if not a:
                continue

            url = a.get("href")
            title = a.get_text(strip=True)

            if not url or not title:
                continue

            # ì ˆëŒ€ URL ë³´ì •
            if url.startswith("//"):
                url = "https:" + url

            if url in seen_urls:
                continue

            seen_urls.add(url)
            articles.append({
                "title": title,
                "url": url,
                "order": len(articles) + 1,
            })

        added = len(articles) - before

        if (click_try + 1) % 10 == 0 or click_try == 0:
            print(f"[ì§„í–‰] ì´ {len(articles)}ê°œ (ì´ë²ˆ ë£¨í”„: +{added}ê°œ)")

        if len(articles) >= target_count:
            break

        if added == 0:
            no_new_rounds += 1
        else:
            no_new_rounds = 0

        # ë”ë³´ê¸° ë²„íŠ¼ í´ë¦­ ì‹œë„
        if no_new_rounds >= 2:
            print("ìƒˆë¡œ ì¶”ê°€ëœ ê¸°ì‚¬ê°€ ì—†ì–´ ì¢…ë£Œ")
            break

        try:
            btn = driver.find_element(
                By.CSS_SELECTOR,
                "a.section_more_inner._CONTENT_LIST_LOAD_MORE_BUTTON"
            )
            driver.execute_script("arguments[0].scrollIntoView(true);", btn)
            time.sleep(0.5)

            try:
                btn.click()
            except Exception:
                driver.execute_script("arguments[0].click();", btn)

            click_try += 1
            time.sleep(2.0)

        except Exception as e:
            print(f"[ê²½ê³ ] ë”ë³´ê¸° ë²„íŠ¼ í´ë¦­ ì‹¤íŒ¨: {repr(e)[:60]}")
            break

    print(f"\nâœ“ URL ìˆ˜ì§‘ ì™„ë£Œ: {len(articles)}ê°œ")
    return articles


def extract_all_contents(driver, articles, max_count=1000):
    """ëª¨ë“  ê¸°ì‚¬ì˜ ë³¸ë¬¸ ì¶”ì¶œ (ìµœëŒ€ ê°œìˆ˜ ì œí•œ)"""
    print("\n" + "="*60)
    print(f"ğŸ“ ë³¸ë¬¸ ì¶”ì¶œ ì‹œì‘ (ëª©í‘œ: {max_count}ê°œ, ìˆ˜ì§‘: {len(articles)}ê°œ)")
    print("="*60)
    
    results = []
    failed_count = 0
    
    for idx, article_info in enumerate(articles):
        # ëª©í‘œ ê°œìˆ˜ ë„ë‹¬ ì‹œ ì¤‘ë‹¨
        if len(results) >= max_count:
            print(f"\nâœ“ ëª©í‘œ ê°œìˆ˜({max_count}ê°œ) ë„ë‹¬, ì¶”ì¶œ ì¢…ë£Œ")
            break
        
        if (idx + 1) % 50 == 0:
            print(f"[{idx+1}/{len(articles)}] ì§„í–‰ ì¤‘... (ì„±ê³µ: {len(results)}, ì‹¤íŒ¨: {failed_count})")
        
        try:
            content, published_at = extract_content(driver, article_info['url'])
            
            if content and len(content) >= 100:
                fetched_at = datetime.now(tz.gettz("Asia/Seoul"))
                
                article_data = {
                    "title": article_info['title'],
                    "url": article_info['url'],
                    "source": extract_source(article_info['url']),
                    "section": "ê²½ì œ",
                    "fetched_at": fetched_at.isoformat(),
                    "content": content,
                    "content_length": len(content),
                }
                
                # published_atì´ ìˆìœ¼ë©´ ì¶”ê°€
                if published_at:
                    article_data["published_at"] = published_at
                
                results.append(article_data)
            else:
                failed_count += 1
        except Exception as e:
            failed_count += 1
            continue
        
        time.sleep(0.3)
    
    print(f"\nâœ“ ë³¸ë¬¸ ì¶”ì¶œ ì™„ë£Œ: {len(results)}ê°œ ì„±ê³µ / {failed_count}ê°œ ì‹¤íŒ¨")
    return results, failed_count


def upload_to_s3(local_file_path, date_obj):
    """S3ì— íŒŒì¼ ì—…ë¡œë“œ (íŒŒí‹°ì…˜ êµ¬ì¡°: year/month/day)"""
    if not s3_client:
        print("\nâš ï¸ boto3ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•„ S3 ì—…ë¡œë“œë¥¼ ê±´ë„ˆëœë‹ˆë‹¤.")
        return False
    
    try:
        year = date_obj.strftime("%Y")
        month = str(int(date_obj.strftime('%m')))
        day = str(int(date_obj.strftime('%d')))
        
        s3_key = f"news-articles/year={year}/month={month}/day={day}/multi_section_top100.json"
        
        print(f"\nğŸ“¤ S3 ì—…ë¡œë“œ ì¤‘...")
        print(f"   s3://{S3_BUCKET_NAME}/{s3_key}")
        
        s3_client.upload_file(
            local_file_path, 
            S3_BUCKET_NAME, 
            s3_key,
            ExtraArgs={'ContentType': 'application/json'}
        )
        
        print(f"âœ… S3 ì—…ë¡œë“œ ì™„ë£Œ!")
        return True
        
    except ClientError as e:
        print(f"âŒ S3 ì—…ë¡œë“œ ì‹¤íŒ¨: {e}")
        return False
    except Exception as e:
        print(f"âŒ S3 ì—…ë¡œë“œ ì¤‘ ì—ëŸ¬: {e}")
        return False


def main():
    import sys
    try:
        sys.stdout.reconfigure(line_buffering=True)
    except Exception:
        pass

    start_time = time.time()
    start_datetime = datetime.now(tz.gettz("Asia/Seoul"))

    print("="*60)
    print("ë„¤ì´ë²„ ë‰´ìŠ¤ ê²½ì œ ì„¹ì…˜ í¬ë¡¤ëŸ¬")
    print(f"Started at: {start_datetime.strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"Target: {TARGET_COUNT}ê°œ ê¸°ì‚¬")
    print("="*60)

    driver = setup_driver()

    try:
        # 1ë‹¨ê³„: ê¸°ì‚¬ URL ìˆ˜ì§‘
        articles = crawl_economy_section(driver, target_count=TARGET_COUNT)
        
        # 2ë‹¨ê³„: ë³¸ë¬¸ ì¶”ì¶œ
        results, failed_count = extract_all_contents(driver, articles, max_count=TARGET_COUNT)
        
    finally:
        driver.quit()
        print("\nğŸ”’ ë¸Œë¼ìš°ì € ì¢…ë£Œ")

    # ì¢…ë£Œ ì‹œê°„
    end_time = time.time()
    end_datetime = datetime.now(tz.gettz("Asia/Seoul"))
    elapsed_seconds = end_time - start_time
    elapsed_minutes = elapsed_seconds / 60

    # ì €ì¥
    fetched_at = datetime.now(tz.gettz("Asia/Seoul"))
    date_folder = fetched_at.strftime("%Y%m%d")
    os.makedirs(f"articles/{date_folder}", exist_ok=True)
    
    # íŒŒì¼ëª…: multi_section_top100.json
    out_filename = "multi_section_top100.json"
    out_path = f"articles/{date_folder}/{out_filename}"

    final_output = {
        "metadata": {
            "start_time": start_datetime.isoformat(),
            "end_time": end_datetime.isoformat(),
            "elapsed_seconds": round(elapsed_seconds, 2),
            "elapsed_minutes": round(elapsed_minutes, 2),
            "total_target": TARGET_COUNT,
            "urls_collected": len(articles),
            "contents_success": len(results),
            "contents_failed": failed_count,
            "section": "ê²½ì œ",
            "source": "ë„¤ì´ë²„ ë‰´ìŠ¤ ì„¹ì…˜"
        },
        "articles": results
    }

    with open(out_path, "w", encoding="utf-8") as f:
        json.dump(final_output, f, ensure_ascii=False, indent=2)

    print("\n" + "="*60)
    print("ğŸ“Š ìµœì¢… ê²°ê³¼")
    print("="*60)
    print(f"ì‹œì‘: {start_datetime.strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"ì¢…ë£Œ: {end_datetime.strftime('%Y-%m-%d %H:%M:%S')}")
    print(f"ì†Œìš” ì‹œê°„: {elapsed_minutes:.2f}ë¶„ ({elapsed_seconds:.2f}ì´ˆ)")
    print(f"\nğŸ“ˆ ê²°ê³¼:")
    print(f"  URL ìˆ˜ì§‘: {len(articles)}ê°œ")
    print(f"  ë³¸ë¬¸ ì„±ê³µ: {len(results)}ê°œ")
    print(f"  ë³¸ë¬¸ ì‹¤íŒ¨: {failed_count}ê°œ")
    print(f"\nğŸ¯ ìµœì¢… ìˆ˜ì§‘: {len(results)}ê°œ")
    print(f"ğŸ’¾ ë¡œì»¬ ì €ì¥: {out_path}")
    
    # S3 ì—…ë¡œë“œ (íŒŒí‹°ì…˜ êµ¬ì¡°)
    upload_to_s3(out_path, fetched_at)
    
    print("="*60)
    
    return final_output


if __name__ == "__main__":
    main()